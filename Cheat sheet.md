Cheat sheet
===========

Index by initials

Graph embedding:

1.Graph embeddings are the transformation of property graphs to a vector or a
set of vectors. Embedding should capture the graph topology, vertex-to-vertex
relationship, and other relevant information about graphs, subgraphs, and
vertices.

2.Here we represent the whole graph with a single vector. Those embeddings are
used when we want to make predictions on the graph level and when we want to
compare or visualize the whole graphs, e.g. comparison of chemical structures.

nll: negative log likelihood loss, i.e. cross-entropy cost function

Node embedding: We encode each vertex (node) with its own vector representation.
We would use this embedding when we want to perform visualization or prediction
on the vertex level, e.g. visualization of vertices in the 2D plane, or
prediction of new connections based on vertex similarities.

Normalization: Normalization and standarization are pretty much the same thing
and both relate to the issue of feature scaling. 

Laplacian matrix:

![IMG_256](media/bd2b7bdeab9615e073bc562217049a0f.png)

Regularization: Regularization is a technique to avoid overfitting when training
machine learning algorithms.

N-th power of the adjacency matrix:
第i行第j个元素表示从i出发走到点j走n步，有多少种走法。



ﬁrst order approximation: 

second order approximation: 

